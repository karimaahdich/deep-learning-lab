# ğŸ§  Deep Learning Project â€” Regression & Multi-Class Classification  
Master SÃ©curitÃ© IT & Big Data â€“ FST Tanger  
**Rapport dÃ©taillÃ© du TP Deep Learning**

---

## ğŸ“Œ Introduction

Ce travail sâ€™inscrit dans le cadre du laboratoire Deep Learning. Lâ€™objectif est dâ€™appliquer diffÃ©rentes techniques de machine learning et de deep learning sur deux problÃ©matiques distinctes :

1. **RÃ©gression** : prÃ©dire le prix de clÃ´ture dâ€™un actif financier.  
2. **Classification multi-classe** : prÃ©dire le type de panne dans un systÃ¨me industriel (Maintenance prÃ©dictive).

Au travers de ces deux projets, plusieurs techniques ont Ã©tÃ© manipulÃ©es :  
âœ” PrÃ©-traitement des donnÃ©es  
âœ” Analyse exploratoire (EDA)  
âœ” Construction de modÃ¨les neuronaux (MLP) sous PyTorch  
âœ” RÃ©gularisation (Dropout, BatchNorm, Weight Decay)  
âœ” Optimisation des hyperparamÃ¨tres (GridSearchCV + Skorch)  
âœ” Visualisation des courbes Loss / Accuracy  
âœ” Calcul et interprÃ©tation des mÃ©triques de performance  

---

# ğŸ§© PARTIE I â€” RÃ‰GRESSION (PrÃ©diction du prix de clÃ´ture)

## 1ï¸âƒ£ PrÃ©-traitement et Analyse Exploratoire (EDA)

### âœ” VÃ©rification de la qualitÃ© des donnÃ©es
- Aucun *manquant*, aucun *doublon*
- Nettoyage des colonnes inutiles

### âœ” Analyse statistique et corrÃ©lations
- CorrÃ©lation quasi parfaite entre `open`, `high`, `low`, `close`
- Volume faiblement corrÃ©lÃ© â†’ information complÃ©mentaire

### âœ” Visualisations
- Histogrammes, heatmap, scatterplots

---

## 2ï¸âƒ£ ModÃ¨le PyTorch (MLP)

- Architecture : 128 â†’ 64 â†’ 32  
- Activation : ReLU  
- Optimizer : Adam  
- Loss : MSELoss  

RÃ©sultat :  
- **TrÃ¨s faible erreur** en train et test  
- ModÃ¨le convergent, pas dâ€™overfitting

---

## 3ï¸âƒ£ Grid Search â€” Optimisation

ParamÃ¨tres testÃ©s : LR, architectures, batch size, epochs, optimizers.  
Meilleurs hyperparamÃ¨tres :  

- **LR = 0.01**  
- **Optimiseur = Adam**  
- **Architecture = [128,64,32]**  
- **Test Loss â‰ˆ 0.000416**

---

## 4ï¸âƒ£ Visualisation â€” Loss et RÂ²

- Courbes Loss montrent une convergence rapide  
- RÂ² trÃ¨s Ã©levÃ© â†’ forte capacitÃ© explicative

---

## 5ï¸âƒ£ RÃ©gularisation

- Dropout  
- Batch Normalization  
- L2 Weight Decay  

â†’ ModÃ¨le plus stable, meilleure gÃ©nÃ©ralisation

---

# ğŸ§© PARTIE II â€” CLASSIFICATION MULTI-CLASSE

## 1ï¸âƒ£ PrÃ©traitement

Classes : Heat Dissipation, No Failure, Overstrain, Power Failure, Random, Tool Wear.

Ã‰tapes :  
âœ” Encodage LabelEncoder  
âœ” Normalisation StandardScaler  
âœ” VÃ©rification outliers/distributions

---

## 2ï¸âƒ£ Analyse Exploratoire

- DÃ©sÃ©quilibre important des classes  
- CorrÃ©lation entre tempÃ©rature & process temp  
- Boxplots par type de panne

---

## 3ï¸âƒ£ Ã‰quilibrage â€” SMOTE

SMOTE a permis dâ€™obtenir un dataset **totalement Ã©quilibrÃ©**, amÃ©liorant la capacitÃ© du modÃ¨le Ã  classifier les classes rares.

---

## 4ï¸âƒ£ ModÃ¨le PyTorch

Architecture finale :  
- 128 â†’ 64 â†’ 32  
- Dropout = 0.2  
- BatchNorm  
- Adam  
- CrossEntropyLoss  

Accuracy test : **97â€“98%**

---

## 5ï¸âƒ£ Grid Search (Skorch)

Meilleurs hyperparamÃ¨tres :  
- LR = 0.001  
- Dropout = 0.2  
- Hidden layers = [128, 64, 32]  
- Batch size = 512  
- Optimizer = Adam  

Cross-val accuracy = **95.88%**

---

## 6ï¸âƒ£ Courbes Loss & Accuracy

- Aucune trace dâ€™overfitting  
- Accuracy validation â‰ˆ 0.98+  
- TrÃ¨s bonne stabilitÃ©

---

## 7ï¸âƒ£ Ã‰valuation

### ğŸ”¥ RÃ©sultats finaux :

| MÃ©trique | Train | Test |
|----------|--------|--------|
| Accuracy | **99.07%** | **98.82%** |
| Precision | 99.09% | 98.84% |
| Recall | 99.07% | 98.82% |
| F1-Score | 99.06% | 98.80% |

La matrice de confusion montre une classification presque parfaite.

---

# ğŸ“ SynthÃ¨se â€” Ce que jâ€™ai appris

- Analyse exploratoire poussÃ©e  
- PrÃ©traitement (normalisation, encodage, SMOTE)  
- ImplÃ©mentation dâ€™un MLP PyTorch  
- RÃ©gularisation et optimisation  
- Ã‰valuation complÃ¨te dâ€™un modÃ¨le DL  
- Visualisation des mÃ©triques  

---

# ğŸ“Œ Conclusion

Le projet a permis dâ€™appliquer toutes les Ã©tapes avancÃ©es du Deep Learning.  
Les rÃ©sultats atteignent jusquâ€™Ã  **99% dâ€™accuracy**, confirmant la qualitÃ© des modÃ¨les dÃ©veloppÃ©s.

---  
